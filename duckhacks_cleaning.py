# -*- coding: utf-8 -*-
"""DuckHacks Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBnr-20b-jR9XKbEx71qYdvmuYfNZUVc
"""

import pandas as pd
import numpy as np
#Reading the toxic comment data downloaded from Kaggle.com
training_data = pd.read_csv("train.csv")

training_data = training_data[['comment_text','toxic']]

#The dataset is heavily unbalanced, we need to balance it first

training_data_toxic = training_data.loc[training_data['toxic'] == 1]

training_data_non_toxic = training_data.loc[training_data['toxic'] == 0]

#Balancing the toxic and non-toxic dataset
training_data_non_toxic = training_data_non_toxic.sample(n=len(training_data_toxic))

# Concatinating the toxic and non-toxic dataset to make a balanced dataset
training_data = pd.concat([training_data_toxic, training_data_non_toxic])

#Randomly shuffling the dataframe
training_data = training_data.sample(frac=1)

#Now our dataset is ready for some Natural Language Processing
def remove_new_lines(row):
  row['comment_text'] = row['comment_text'].replace("\n","")
  return row['comment_text']

training_data['comment_text'] = training_data.apply (lambda row: remove_new_lines(row),axis=1)

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
# Removing stop words
def remove_stop_words(row):
  example = row['comment_text']
  stop_words = set(stopwords.words('english')) 
  word_tokens = word_tokenize(example)
  filtered_sentence = [] 
  for w in word_tokens: 
    if w not in stop_words: 
        filtered_sentence.append(w)
  return ' '.join(filtered_sentence)

training_data['comment_text'] = training_data.apply(lambda row: remove_stop_words(row),axis=1)

training_data = training_data.reset_index()

training_data = training_data.drop(['index'],axis=1)

def strip(row):
  row['comment_text'] = row['comment_text'].strip()
  return row['comment_text']

training_data['comment_text'] = training_data.apply(lambda row: strip(row),axis=1)

def remove_weird(row):
  row['comment_text'] = row['comment_text'].replace("``","")
  row['comment_text'] = row['comment_text'].replace('""',"")
  row['comment_text'] = row['comment_text'].replace("''","")
  row['comment_text'] = row['comment_text'].replace("  "," ")
  return row['comment_text']

training_data['comment_text'] = training_data.apply(lambda row: remove_weird(row),axis=1)

# Cleaned Data
training_data.head()

# Making a new CSV File to save the cleaned data'

training_data.to_csv("cleaned.csv",index=False)